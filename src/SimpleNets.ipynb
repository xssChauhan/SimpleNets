{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Data Loader\n",
    "\n",
    "def load_data(file = \"data/mnist.pkl.gz\"):\n",
    "    f = gzip.open(file,\"rb\")\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = \"latin1\"\n",
    "    p = u.load()\n",
    "    return p\n",
    "\n",
    "def vectorized_results(j):\n",
    "    e = np.zeros((10,1))\n",
    "    e[j] = 1\n",
    "    return e\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d , va_d , te_d = load_data()\n",
    "    training_inputs = [\n",
    "        np.reshape(x,(784,1)) for x in tr_d[0]\n",
    "    ]\n",
    "    training_results = [\n",
    "        vectorized_results(y) for y in tr_d[1]\n",
    "    ]\n",
    "    training_data = zip(\n",
    "        training_inputs , training_results\n",
    "    )\n",
    "    validation_inputs = [\n",
    "        np.reshape(x,(784,1)) for x in va_d[0]\n",
    "    ]\n",
    "    validation_data = zip(\n",
    "        validation_inputs , va_d[1]\n",
    "    )\n",
    "    test_inputs = [\n",
    "        np.reshape(x , (784,1)) for x in te_d[0]\n",
    "    ]\n",
    "    test_data = zip(test_inputs , te_d[1])\n",
    "    return list(training_data) , np.array(list(validation_data)) , np.array(list(test_data))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost Definitions\n",
    "\n",
    "class QuadraticCost:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(output , input):\n",
    "        return 0.5 * np.linalg.norm(y-a)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(activation , z, output):\n",
    "        return (activation - output)*sigmoid_prime(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseNetwork:\n",
    "    \n",
    "    def __init__(self , size , cost = QuadraticCost , batch_size = 1):\n",
    "        self.size = size\n",
    "        self.num_layers = len(size)\n",
    "        self.cost = cost\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = self.initialize_weights()\n",
    "        self.biases = self.initialize_biases()\n",
    "        \n",
    "    def _get_weight_dimensions(self):\n",
    "        '''\n",
    "        Return an iterable with the sizes of weights for each layer\n",
    "        '''\n",
    "        return zip(\n",
    "            self.size[:-1] , self.size[1:]\n",
    "        )\n",
    "    def _get_bias_dimensions(self):\n",
    "        '''\n",
    "        Return an iterable with the sizes of the biases for each layer\n",
    "        '''\n",
    "        return self.size[1:]\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        '''\n",
    "        Initialize the weights using Gaussian distribution with mean 0\n",
    "        and sd 1 over the square root of the number of weights connectin\n",
    "        to the same neuron\n",
    "        '''\n",
    "        sizes = self._get_weight_dimensions()\n",
    "        \n",
    "        return [\n",
    "            np.random.randn(y,x)/np.sqrt(x)\n",
    "            for x,y in sizes\n",
    "        ]\n",
    "    \n",
    "    def initialize_biases(self):\n",
    "        size = self._get_bias_dimensions()\n",
    "        return [\n",
    "            np.random.randn(self.batch_size , y,1) for y in size\n",
    "        ]\n",
    "    \n",
    "    def activation(self , w , b,a):\n",
    "        return sigmoid(\n",
    "            np.matmul(w,a) + b\n",
    "        )\n",
    "        \n",
    "    def inspect(self):\n",
    "        print(\"Weights\")\n",
    "        print([np.shape(e) for e in self.weights ])\n",
    "        print(\"Biases\")\n",
    "        print([np.shape(e) for e in self.biases ])\n",
    "        \n",
    "    def feedforward(self , x):\n",
    "        a = x\n",
    "        for w,b in zip(self.weights , self.biases):\n",
    "            a = self.activation(w,b,a)\n",
    "        return a\n",
    "    \n",
    "    def backprop(self , x , y):\n",
    "        '''\n",
    "        Implement the backprop Pipeline\n",
    "        1. Feedforward with the x,y\n",
    "        2. Calculate backward errors at each step\n",
    "        '''\n",
    "        nabla_b = [\n",
    "            np.zeros(b.shape) for b in self.biases\n",
    "        ]\n",
    "        nabla_w = [\n",
    "            np.zeros(w.shape) for w in self.weights\n",
    "        ]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "    \n",
    "        for b,w in zip(self.biases, self.weights):\n",
    "            z = self.activation(w,b,activation)\n",
    "            activation = sigmoid(z)\n",
    "            zs.append(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        #At this point I have all the zs and activations\n",
    "        \n",
    "        #Error at the output layer\n",
    "        delta = (self.cost).delta(\n",
    "            activations[-1] , zs[-1] , y\n",
    "        )\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.matmul(\n",
    "            delta , np.transpose(activations[-2]  ,axes = [0,2,1])\n",
    "        )\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.matmul(\n",
    "                np.transpose(\n",
    "                    self.weights[-l+1]\n",
    "                ),\n",
    "                delta\n",
    "            ) * sigmoid_prime(zs[-l])\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.matmul(\n",
    "                delta,\n",
    "                np.transpose(\n",
    "                    activations[-l-1]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #Now that we have the error at the output layer, we backpropagate it\n",
    "        \n",
    "        return nabla_b , nabla_w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[[ 0.00631094],\n",
       "          [ 0.02475288],\n",
       "          [-0.0158375 ],\n",
       "          [-0.01633883],\n",
       "          [-0.01199943],\n",
       "          [ 0.014742  ],\n",
       "          [ 0.0140955 ],\n",
       "          [ 0.03165616],\n",
       "          [-0.0272028 ],\n",
       "          [-0.01774997],\n",
       "          [-0.01978159],\n",
       "          [ 0.00028084],\n",
       "          [ 0.02080956],\n",
       "          [ 0.01393356],\n",
       "          [ 0.0108351 ],\n",
       "          [-0.01732825],\n",
       "          [-0.01575855],\n",
       "          [-0.01584315],\n",
       "          [-0.00773702],\n",
       "          [-0.00615239],\n",
       "          [-0.03207866],\n",
       "          [ 0.01582908],\n",
       "          [ 0.00891985],\n",
       "          [ 0.00359077],\n",
       "          [-0.00742408],\n",
       "          [ 0.04956248],\n",
       "          [-0.01392205],\n",
       "          [ 0.00107689],\n",
       "          [ 0.00474457],\n",
       "          [-0.0053545 ]]]), array([[[ 0.14755358],\n",
       "          [ 0.1307305 ],\n",
       "          [ 0.14803519],\n",
       "          [ 0.14810396],\n",
       "          [ 0.13394327],\n",
       "          [-0.10353604],\n",
       "          [ 0.14787448],\n",
       "          [ 0.14767859],\n",
       "          [ 0.14789961],\n",
       "          [ 0.14017782]]])], [array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          ..., \n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0., ...,  0.,  0.,  0.]]]),\n",
       "  array([[[ 0.09289134,  0.09394139,  0.10402863,  0.08004005,  0.09418641,\n",
       "            0.08349262,  0.07682485,  0.09441797,  0.1007158 ,  0.07945354,\n",
       "            0.08570249,  0.1037444 ,  0.09882641,  0.10061031,  0.10213899,\n",
       "            0.08536842,  0.10264023,  0.08953802,  0.08803889,  0.09463759,\n",
       "            0.09699625,  0.09005217,  0.09068883,  0.09640666,  0.102236  ,\n",
       "            0.10227073,  0.09056531,  0.10251183,  0.09262226,  0.08217507],\n",
       "          [ 0.08230048,  0.08323082,  0.09216798,  0.07091442,  0.0834479 ,\n",
       "            0.07397335,  0.0680658 ,  0.08365306,  0.08923285,  0.07039478,\n",
       "            0.07593126,  0.09191616,  0.08755888,  0.08913939,  0.09049379,\n",
       "            0.07563529,  0.09093787,  0.07932949,  0.07800128,  0.08384764,\n",
       "            0.08593738,  0.07978502,  0.08034909,  0.08541502,  0.09057973,\n",
       "            0.0906105 ,  0.08023966,  0.09082411,  0.08206209,  0.07280602],\n",
       "          [ 0.09319454,  0.09424802,  0.10436818,  0.0803013 ,  0.09449384,\n",
       "            0.08376514,  0.07707561,  0.09472615,  0.10104454,  0.07971288,\n",
       "            0.08598223,  0.10408303,  0.09914898,  0.10093871,  0.10247238,\n",
       "            0.08564707,  0.10297524,  0.08983027,  0.08832625,  0.09494649,\n",
       "            0.09731284,  0.0903461 ,  0.09098484,  0.09672134,  0.1025697 ,\n",
       "            0.10260454,  0.09086092,  0.10284643,  0.09292458,  0.0824433 ],\n",
       "          [ 0.09323783,  0.0942918 ,  0.10441666,  0.0803386 ,  0.09453773,\n",
       "            0.08380405,  0.07711142,  0.09477015,  0.10109147,  0.0797499 ,\n",
       "            0.08602217,  0.10413137,  0.09919504,  0.10098559,  0.10251998,\n",
       "            0.08568685,  0.10302308,  0.089872  ,  0.08836728,  0.09499059,\n",
       "            0.09735805,  0.09038807,  0.0910271 ,  0.09676626,  0.10261735,\n",
       "            0.1026522 ,  0.09090312,  0.1028942 ,  0.09296775,  0.08248159],\n",
       "          [ 0.08432306,  0.08527626,  0.09443306,  0.07265718,  0.08549868,\n",
       "            0.07579128,  0.06973855,  0.08570888,  0.0914258 ,  0.07212476,\n",
       "            0.07779732,  0.09417504,  0.08971069,  0.09133004,  0.09271772,\n",
       "            0.07749406,  0.09317272,  0.08127905,  0.0799182 ,  0.08590824,\n",
       "            0.08804934,  0.08174578,  0.08232372,  0.08751414,  0.09280578,\n",
       "            0.0928373 ,  0.08221159,  0.09305616,  0.08407881,  0.07459527],\n",
       "          [-0.0651804 , -0.06591721, -0.07299527, -0.05616286, -0.06608913,\n",
       "           -0.05858547, -0.0539068 , -0.06625162, -0.07067071, -0.05575131,\n",
       "           -0.0601361 , -0.07279583, -0.06934495, -0.07059669, -0.07166934,\n",
       "           -0.05990169, -0.07202105, -0.06282743, -0.06177552, -0.06640572,\n",
       "           -0.06806075, -0.0631882 , -0.06363494, -0.06764705, -0.07173741,\n",
       "           -0.07176177, -0.06354827, -0.07193095, -0.0649916 , -0.05766097],\n",
       "          [ 0.09309336,  0.0941457 ,  0.10425488,  0.08021413,  0.09439125,\n",
       "            0.08367421,  0.07699194,  0.09462331,  0.10093484,  0.07962634,\n",
       "            0.08588888,  0.10397003,  0.09904135,  0.10082912,  0.10236113,\n",
       "            0.08555409,  0.10286345,  0.08973275,  0.08823036,  0.09484341,\n",
       "            0.0972072 ,  0.09024802,  0.09088607,  0.09661633,  0.10245835,\n",
       "            0.10249315,  0.09076228,  0.10273478,  0.0928237 ,  0.08235379],\n",
       "          [ 0.09297004,  0.09402098,  0.10411677,  0.08010786,  0.09426621,\n",
       "            0.08356336,  0.07688994,  0.09449796,  0.10080113,  0.07952086,\n",
       "            0.0857751 ,  0.1038323 ,  0.09891014,  0.10069555,  0.10222553,\n",
       "            0.08544075,  0.10272719,  0.08961388,  0.08811348,  0.09471777,\n",
       "            0.09707843,  0.09012847,  0.09076567,  0.09648834,  0.10232262,\n",
       "            0.10235737,  0.09064204,  0.10259868,  0.09270074,  0.0822447 ],\n",
       "          [ 0.09310918,  0.0941617 ,  0.10427259,  0.08022776,  0.09440729,\n",
       "            0.08368842,  0.07700502,  0.09463939,  0.10095199,  0.07963987,\n",
       "            0.08590348,  0.1039877 ,  0.09905818,  0.10084626,  0.10237852,\n",
       "            0.08556863,  0.10288093,  0.089748  ,  0.08824535,  0.09485953,\n",
       "            0.09722372,  0.09026336,  0.09090151,  0.09663275,  0.10247576,\n",
       "            0.10251057,  0.0907777 ,  0.10275223,  0.09283948,  0.08236779],\n",
       "          [ 0.08824798,  0.08924555,  0.09882856,  0.07603909,  0.08947832,\n",
       "            0.07931908,  0.07298462,  0.0896983 ,  0.09568132,  0.0754819 ,\n",
       "            0.08141849,  0.09855854,  0.09388638,  0.09558111,  0.09703338,\n",
       "            0.08110112,  0.09750955,  0.08506229,  0.0836381 ,  0.08990695,\n",
       "            0.0921477 ,  0.08555074,  0.08615557,  0.09158759,  0.09712554,\n",
       "            0.09715852,  0.08603823,  0.09738757,  0.08799236,  0.07806739]]])])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_d , va_d , te_d = load_data_wrapper()\n",
    "net = BaseNetwork([784 , 30 , 10] , batch_size = 1)\n",
    "\n",
    "net.backprop(tr_d[0][0] , tr_d[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-024e78d3c482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(10,20)\n",
    "b = np.random.randn(20,30)\n",
    "\n",
    "np.matmul(net.weights[0] , batch[:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(1,10,1)\n",
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 10)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.transpose(a , axes = [0,2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
